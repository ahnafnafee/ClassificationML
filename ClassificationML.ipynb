{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "06bd6e6bb8b1a59ff9a8fa9390517002ce8a147f96d75124d9423e09c7a6bc86"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Q1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "from scipy.linalg import svd\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as la\n",
    "\n",
    "x = np.array([[-2], [-5], [-3], [0], [-8], [-2], [1], [5], [-1], [6]])\n",
    "y = np.array([[1], [-4], [1], [3], [11], [5], [0], [-1], [-3], [1]])\n",
    "\n",
    "def lse_1(x,y):\n",
    "    biasF = np.ones(((len(x)), 1))\n",
    "    X = np.hstack((biasF, x))\n",
    "\n",
    "    w = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "\n",
    "    return w\n",
    "    \n",
    "\n",
    "w = lse_1(x,y)\n",
    "\n",
    "Y = np.zeros(shape=(10,1))\n",
    "\n",
    "for j in range(len(y)):\n",
    "    Y[j] = (w[1]*x[j] + w[0])\n",
    "\n",
    "print(\"Y_pred: \\n\", Y)\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(np.mean((predictions-targets)**2))\n",
    "\n",
    "rootmean = rmse(Y, y)\n",
    "print(\"RMSE: \", rmse(Y, y))"
   ]
  },
  {
   "source": [
    "Q2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc\n",
    "import matplotlib.cm as cm\n",
    "from collections import Counter\n",
    "import glob\n",
    "from matplotlib.image import imread\n",
    "from enum import Enum\n",
    "from scipy.linalg import svd\n",
    "import scipy.stats as ss\n",
    "import collections\n",
    "\n",
    "# For testing\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "imp_data = np.genfromtxt('spambase.data', delimiter=',')\n",
    "\n",
    "def train_test_split(data, train_size, random_state):\n",
    "    '''Splitting testing and training data'''\n",
    "\n",
    "    # Resetting random seed\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    # Rows shuffled\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Calculates array index for splitting\n",
    "    spltIdx = int(np.ceil((2/3)*n))\n",
    "\n",
    "    # Training-validation data split\n",
    "    data_train, data_test = data[:spltIdx,:], data[spltIdx:,:]\n",
    "\n",
    "    # Training data\n",
    "    x_tr, y_tr = np.hsplit(data_train, [-1])\n",
    "    # Testing Data\n",
    "    x_tt, y_tt = np.hsplit(data_test, [-1])\n",
    "\n",
    "\n",
    "\n",
    "    # Separating class label from data\n",
    "    class_label_tr = data_train[:, -1].astype(int)\n",
    "    dataset_tr = data_train[:, :-1]\n",
    "\n",
    "    class_label_tt = data_test[:, -1].astype(int)\n",
    "    dataset_tt = data_test[:, :-1]\n",
    "\n",
    "    # Filtering features with low std\n",
    "    # dataset_tr = std_filter1(dataset_tr, 0)\n",
    "    # dataset_tt = std_filter1(dataset_tt, 0)\n",
    "\n",
    "    og_mean = np.mean(dataset_tr)\n",
    "    og_std = np.std(dataset_tr)\n",
    "\n",
    "    # dataset_tr = (dataset_tr - np.mean(dataset_tr)) / np.std(dataset_tr)\n",
    "    # dataset_tt = (dataset_tt - np.mean(dataset_tt)) / np.std(dataset_tt)\n",
    "\n",
    "    dataset_tr = (dataset_tr - og_mean) / og_std\n",
    "    dataset_tt = (dataset_tt - og_mean) / og_std\n",
    "\n",
    "    # x_tr = (x_tr - np.mean(x_tr)) / np.std(x_tr)\n",
    "    # x_tt = (x_tt - np.mean(x_tt)) / np.std(x_tt)\n",
    "\n",
    "    # return x_tr, y_tr, x_tt, y_tt\n",
    "    return dataset_tr, class_label_tr, dataset_tt, class_label_tt\n",
    "    # return dataset_tr, y_tr, dataset_tt, y_tt\n",
    "\n",
    "\n",
    "def std_filter(data, std_val):\n",
    "    '''Filters out features with low std'''\n",
    "\n",
    "    dataset = np.copy(data)\n",
    "    temp = 0\n",
    "    while temp < dataset.shape[1]:\n",
    "        if(np.std(dataset[:,temp]) == 0):\n",
    "            dataset = np.delete(dataset, temp, 1)\n",
    "            temp = temp - 1\n",
    "        else:\n",
    "            dataset[:,temp] = (dataset[:,temp] - np.mean(dataset[:,temp])) / np.std(dataset[:,temp])\n",
    "            temp = temp + 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierEvaluation:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true.astype(int)\n",
    "        self.y_pred = y_pred.astype(int)\n",
    "        unique, counts = np.unique(y_true, return_counts=True)\n",
    "        self.y_true_dict = dict(zip(unique, counts))\n",
    "        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "        self.y_pred_dict = dict(zip(unique, counts))\n",
    "        \n",
    "\n",
    "    def eval(self):\n",
    "        self.TP = 0\n",
    "        self.TN = 0\n",
    "        self.FP = 0\n",
    "        self.FN = 0\n",
    "\n",
    "        for i in range(len(self.y_true)):\n",
    "            if (self.y_true[i] == 1 and self.y_pred[i] == 1):\n",
    "                self.TP += 1\n",
    "            elif (self.y_true[i] == 1 and self.y_pred[i] == 0):\n",
    "                self.FP += 1\n",
    "            elif (self.y_true[i] == 0 and self.y_pred[i] == 1):\n",
    "                self.FN += 1\n",
    "            elif (self.y_true[i] == 0 and self.y_pred[i] == 0):\n",
    "                self.TN += 1\n",
    "            \n",
    "\n",
    "    def get_precision(self):\n",
    "        '''Precision = TP / (TP + FP)'''\n",
    "        \n",
    "        precision = self.TP/(self.TP + self.FP)\n",
    "        return precision\n",
    "\n",
    "    def get_recall(self):\n",
    "        '''Precision = TP / (TP + FN)'''\n",
    "\n",
    "        recall = self.TP/(self.TP + self.FN)\n",
    "        return recall\n",
    "\n",
    "    def get_fmeasure(self):\n",
    "        '''Recall = (2 * Precision * Recall) / (Precision + Recall)'''\n",
    "\n",
    "        fmeasure = (2 * self.get_precision() * self.get_recall())/(self.get_precision() + self.get_recall())\n",
    "        return fmeasure\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        '''Accuracy = (TP + TN) / (TP + TN + FP + FN)'''\n",
    "\n",
    "        accuracy = (self.TP + self.TN) /(self.TP + self.TN + self.FP + self.FN)\n",
    "\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, x, y):\n",
    "        self.n_samples, self.n_features = x.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_classes = len(self.classes)\n",
    "\n",
    "        self.features = x\n",
    "        self.target = y.flatten()\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean_data = np.zeros((self.n_classes, self.n_features), dtype=np.float64)\n",
    "        self.std_data = np.zeros((self.n_classes, self.n_features), dtype=np.float64)\n",
    "        self.prior_data = np.zeros(self.n_classes)\n",
    "        \n",
    "\n",
    "    def get_target(self):\n",
    "        '''For debugging purposes'''\n",
    "\n",
    "        print(np.mean(self.mean_data.flatten()))\n",
    "        print(np.mean(self.std_data.flatten()))\n",
    "        print(self.target.shape)\n",
    "        print(self.features.shape)\n",
    "        return self.target\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "        '''Separates spam and not spam rows'''\n",
    "\n",
    "        data = self.features\n",
    "        label = self.target\n",
    "\n",
    "        label = label.reshape(label.shape[0], 1)\n",
    "\n",
    "        spIdx_lst = np.where(~label.any(axis=1))[0]\n",
    "        notIdx_sp_lst = np.where(label.any(axis=1))[0]\n",
    "\n",
    "        d_list = data.tolist()\n",
    "        sp_list = []\n",
    "        not_sp_list = []\n",
    "\n",
    "        for index in spIdx_lst:\n",
    "            sp_list += [d_list[index]]\n",
    "\n",
    "        for index in notIdx_sp_lst:\n",
    "            not_sp_list += [d_list[index]]\n",
    "\n",
    "\n",
    "        sp_data = np.asarray(sp_list)\n",
    "        not_sp_data = np.asarray(not_sp_list)\n",
    "\n",
    "\n",
    "        self.mean_data[0, :] = sp_data.mean(axis=0)\n",
    "        self.std_data[0, :] = sp_data.std(axis=0)\n",
    "        self.prior_data[0] = sp_data.shape[0] / float(self.n_samples)\n",
    "\n",
    "        self.mean_data[1, :] = not_sp_data.mean(axis=0)\n",
    "        self.std_data[1, :] = not_sp_data.std(axis=0)\n",
    "        self.prior_data[1] = not_sp_data.shape[0] / float(self.n_samples)\n",
    "\n",
    "\n",
    "    def get_stats(self):\n",
    "        '''For debugging purposes'''\n",
    "\n",
    "        return np.sum(np.mean(self.features, axis=0))\n",
    "\n",
    "    \n",
    "    def calc_posterior1(self, x):\n",
    "        '''Chooses the class label based on which class probability is higher'''\n",
    "\n",
    "        posteriors = []\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            prior = np.log(self.prior_data[i])\n",
    "            n_log = np.log(self.norm_pdf(x, i))\n",
    "            n_log = np.nan_to_num(n_log, nan=10^-8, posinf=10^8, neginf=10^-14)\n",
    "            posterior = np.sum(n_log)\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "\n",
    "    def calc_posterior(self, x):\n",
    "        '''Chooses the class label based on which class probability is higher'''\n",
    "\n",
    "        posteriors = []\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            prior = self.prior_data[i]\n",
    "            n_pdf = self.norm_pdf(x, i)\n",
    "            n_pdf = np.prod(np.nan_to_num(n_pdf, nan=10^-8, posinf=10^8, neginf=10^-8))\n",
    "            posterior = prior * n_pdf\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        '''Gets the predicted target values'''\n",
    "\n",
    "        preds = [self.calc_posterior(i) for i in x]\n",
    "        return np.asarray(preds, dtype=np.float64)\n",
    "\n",
    "\n",
    "    def norm_pdf(self, data, c_idx):\n",
    "        '''Calculates norm pdf'''\n",
    "\n",
    "        mean = self.mean_data[c_idx]\n",
    "        std = self.std_data[c_idx]\n",
    "\n",
    "        numerator = np.exp(- (data-mean)**2 / (2 * (std**2)))\n",
    "        denominator = std * np.sqrt(2 * np.pi)\n",
    "\n",
    "        return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Precision: 95.22998296422487%\nRecall: 68.50490196078431%\nF-measure: 79.6863863150392%\nAccuracy: 81.40900195694715%\n"
     ]
    }
   ],
   "source": [
    "x_tr, y_tr, x_tt, y_tt = train_test_split(imp_data, train_size=2/3, random_state=0)\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "# np.set_printoptions(threshold = False)\n",
    "\n",
    "g_nb = NaiveBayes(x_tr, y_tr)\n",
    "g_nb.fit()\n",
    "# print(g_nb.get_stats())\n",
    "predictions = g_nb.predict(x_tt)\n",
    "gb_ce = ClassifierEvaluation(y_tt, predictions)\n",
    "gb_ce.eval()\n",
    "print(f\"Precision: {gb_ce.get_precision() * 100}%\")\n",
    "print(f\"Recall: {gb_ce.get_recall() * 100}%\")\n",
    "print(f\"F-measure: {gb_ce.get_fmeasure() * 100}%\")\n",
    "print(f\"Accuracy: {gb_ce.get_accuracy() * 100}%\")"
   ]
  },
  {
   "source": [
    "### Q3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-554-7f08e6299cda>:38: RuntimeWarning: overflow encountered in exp\n",
      "  ll = np.sum( self.target * scores - np.log(1 + np.exp(scores)) )\n",
      "<ipython-input-554-7f08e6299cda>:33: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1 + np.exp(- x))\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-inf\n",
      "-inf\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-554-7f08e6299cda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[0mg_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m \u001b[0mg_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;31m# g_nb.fit()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m# predictions = g_nb.predict(x_tt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-554-7f08e6299cda>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, max_iters=100000):\n",
    "        self.lr = lr\n",
    "        self.precision = 2**-32\n",
    "        self.max_iters = max_iters\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.features = x\n",
    "        self.target = y\n",
    "\n",
    "        biasF = np.ones((self.features.shape[0], 1))\n",
    "        self.features = np.hstack((biasF, self.features))\n",
    "\n",
    "        self.weights = np.zeros(self.features.shape[1])\n",
    "    \n",
    "        for step in range(self.max_iters):\n",
    "            scores = np.dot(self.features, self.weights)\n",
    "            predictions = self.sigmoid(scores)\n",
    "\n",
    "            # Update weights with gradient\n",
    "            output_error_signal = self.target - predictions\n",
    "            gradient = np.dot(self.features.T, output_error_signal)\n",
    "            self.weights += self.lr * gradient\n",
    "            \n",
    "            # Print log-likelihood every so often\n",
    "            if step % 10000 == 0:\n",
    "                print(self.log_likelihood())\n",
    "\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1 + np.exp(- x))\n",
    "\n",
    "\n",
    "    def log_likelihood(self):\n",
    "        scores = np.dot(self.features, self.weights)\n",
    "        ll = np.sum( self.target * scores - np.log(1 + np.exp(scores)) )\n",
    "        return ll\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def pre_processing(matrix):\n",
    "        matrix_copy = matrix     \n",
    "        b = np.apply_along_axis(lambda x: (x-np.mean(x))/float(np.std(x)),0,matrix_copy)\n",
    "        return b\n",
    "        \n",
    "\n",
    "    def cost_function(X, y, theta):\n",
    "        h_theta = sigmoid(np.dot(X, theta))\n",
    "        log_l = (-y)*np.log(h_theta) + (1 - y)*np.log(1 - h_theta)\n",
    "        return log_l.mean()\n",
    "\n",
    "    def calculate_gradient(X, y, theta, index, X_count):\n",
    "        temp_theta = sigmoid(np.dot(X, theta))\n",
    "        sum_ = 0.0\n",
    "        for i in range(dummy_theta.shape[0]):\n",
    "            sum_ = sum_ + (temp_theta[i] - y[i]) * X[i][index]\n",
    "        return sum_\n",
    "\n",
    "\n",
    "    def gradient_descent(training_set, alpha, max_iterations):\n",
    "        iter_count = 0\n",
    "        training_set = np.asarray(training_set)\n",
    "        X = training_set.T[0:57].T\n",
    "        y = training_set.T[57].T\n",
    "        X_count = X.shape[1]\n",
    "\n",
    "        theta = np.zeros(X_count)\n",
    "        x_vals = []\n",
    "        y_vals = []\n",
    "        regularization_parameter = 1\n",
    "        while(iter_count < max_iterations):\n",
    "            iter_count += 1\n",
    "            for i in range(X_count):\n",
    "                prediction = calculate_gradient(X, y, theta, i, X_count)\n",
    "                prev_theta = theta[i]\n",
    "                if i != 0:\n",
    "                    prediction += (regularization_parameter/X_count)*prev_theta\n",
    "                theta[i] = prev_theta - alpha * prediction\n",
    "\n",
    "\n",
    "\n",
    "x_tr, y_tr, x_tt, y_tt = train_test_split(imp_data, train_size=2/3, random_state=0)\n",
    "\n",
    "g_lr = LogisticRegression()\n",
    "g_lr.fit(x_tr, y_tr)\n",
    "# g_nb.fit()\n",
    "# predictions = g_nb.predict(x_tt)\n",
    "# gb_ce = ClassifierEvaluation(y_tt, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 546
    }
   ],
   "source": [
    "0.0 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}