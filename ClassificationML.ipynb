{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "06bd6e6bb8b1a59ff9a8fa9390517002ce8a147f96d75124d9423e09c7a6bc86"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Q1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "from scipy.linalg import svd\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as la\n",
    "\n",
    "x = np.array([[-2], [-5], [-3], [0], [-8], [-2], [1], [5], [-1], [6]])\n",
    "y = np.array([[1], [-4], [1], [3], [11], [5], [0], [-1], [-3], [1]])\n",
    "\n",
    "def lse_1(x,y):\n",
    "    biasF = np.ones(((len(x)), 1))\n",
    "    X = np.hstack((biasF, x))\n",
    "\n",
    "    w = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "\n",
    "    return w\n",
    "    \n",
    "\n",
    "w = lse_1(x,y)\n",
    "\n",
    "Y = np.zeros(shape=(10,1))\n",
    "\n",
    "for j in range(len(y)):\n",
    "    Y[j] = (w[1]*x[j] + w[0])\n",
    "\n",
    "print(\"Y_pred: \\n\", Y)\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(np.mean((predictions-targets)**2))\n",
    "\n",
    "rootmean = rmse(Y, y)\n",
    "print(\"RMSE: \", rmse(Y, y))"
   ]
  },
  {
   "source": [
    "Q2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Q3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n(2, 57)\n(-0.13402127271738473, -0.13193207910879318)\nLEN:  [0. 0. 0. ... 1. 0. 1.]\nPrecision: 0.9791666666666666\nRecall: 0.6322869955156951\nF-measure 0.768392370572207\nAccuracy: 0.7782126549249837\n\n\nLEN:  1533\n\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8127853881278538"
      ]
     },
     "metadata": {},
     "execution_count": 351
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc\n",
    "import matplotlib.cm as cm\n",
    "from collections import Counter\n",
    "import glob\n",
    "from matplotlib.image import imread\n",
    "from enum import Enum\n",
    "from scipy.linalg import svd\n",
    "import scipy.stats as ss\n",
    "import collections\n",
    "\n",
    "# For testing\n",
    "import sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "imp_data = np.genfromtxt('spambase.data', delimiter=',')\n",
    "\n",
    "def train_test_split(data, train_size, random_state):\n",
    "    '''Splitting testing and training data'''\n",
    "\n",
    "    # Resetting random seed\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    # Rows shuffled\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Calculates array index for splitting\n",
    "    spltIdx = int(np.ceil((2/3)*n))\n",
    "\n",
    "    # Training-validation data split\n",
    "    data_train, data_test = data[:spltIdx,:], data[spltIdx:,:]\n",
    "\n",
    "    # Training data\n",
    "    x_tr, y_tr = np.hsplit(data_train, [-1])\n",
    "    # Testing Data\n",
    "    x_tt, y_tt = np.hsplit(data_test, [-1])\n",
    "\n",
    "\n",
    "\n",
    "    # Separating class label from data\n",
    "    class_label_tr = data_train[:, -1].astype(int)\n",
    "    dataset_tr = data_train[:, :-1]\n",
    "\n",
    "    class_label_tt = data_test[:, -1].astype(int)\n",
    "    dataset_tt = data_test[:, :-1]\n",
    "\n",
    "    # Filtering features with low std\n",
    "    # dataset_tr = std_filter1(dataset_tr, 0)\n",
    "    # dataset_tt = std_filter1(dataset_tt, 0)\n",
    "\n",
    "    og_mean = np.mean(dataset_tr)\n",
    "    og_std = np.std(dataset_tr)\n",
    "\n",
    "    # dataset_tr = (dataset_tr - np.mean(dataset_tr)) / np.std(dataset_tr)\n",
    "    # dataset_tt = (dataset_tt - np.mean(dataset_tt)) / np.std(dataset_tt)\n",
    "\n",
    "    dataset_tr = (dataset_tr - og_mean) / og_std\n",
    "    dataset_tt = (dataset_tt - og_mean) / og_std\n",
    "\n",
    "    # x_tr = (x_tr - np.mean(x_tr)) / np.std(x_tr)\n",
    "    # x_tt = (x_tt - np.mean(x_tt)) / np.std(x_tt)\n",
    "\n",
    "    # return x_tr, y_tr, x_tt, y_tt\n",
    "    return dataset_tr, class_label_tr, dataset_tt, class_label_tt\n",
    "    # return dataset_tr, y_tr, dataset_tt, y_tt\n",
    "\n",
    "\n",
    "def std_filter(data, std_val):\n",
    "    '''Filters out features with low std'''\n",
    "\n",
    "    # Standardizing the matrix\n",
    "    \n",
    "    temp_data = np.copy(data)\n",
    "    std_mat = np.std(temp_data, axis = 0)\n",
    "    col_num = temp_data.shape[1]\n",
    "    low_idx = []\n",
    "\n",
    "    for i in range(col_num):\n",
    "        if std_mat[i] <= std_val:\n",
    "            low_idx.append(i)\n",
    "\n",
    "    temp = 0\n",
    "    for j in low_idx:\n",
    "        temp_data = np.delete(temp_data, j - temp, 1)\n",
    "        temp += 1\n",
    "    \n",
    "    standardized_mat = (temp_data - np.mean(temp_data)) / np.std(temp_data)\n",
    "    return standardized_mat\n",
    "\n",
    "def std_filter1(data, std_val):\n",
    "    '''Filters out features with low std'''\n",
    "\n",
    "    \n",
    "    dataset = np.copy(data)\n",
    "    x = 0\n",
    "    while x < dataset.shape[1]:\n",
    "        if(np.std(dataset[:,x]) == 0):\n",
    "            dataset = np.delete(dataset,x,1)\n",
    "            x = x - 1\n",
    "        else:\n",
    "            dataset[:,x] = (dataset[:,x] - np.mean(dataset[:,x])) / np.std(dataset[:,x])\n",
    "            x = x + 1\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class ClassifierEvaluation:\n",
    "    def __init__(self, y_true, y_pred):\n",
    "        self.y_true = y_true.astype(int)\n",
    "        self.y_pred = y_pred.astype(int)\n",
    "        unique, counts = np.unique(y_true, return_counts=True)\n",
    "        self.y_true_dict = dict(zip(unique, counts))\n",
    "        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "        self.y_pred_dict = dict(zip(unique, counts))\n",
    "        \n",
    "\n",
    "    def eval(self):\n",
    "        self.TP = 0\n",
    "        self.TN = 0\n",
    "        self.FP = 0\n",
    "        self.FN = 0\n",
    "\n",
    "        for i in range(len(self.y_true)):\n",
    "            if (self.y_true[i] == 1 and self.y_pred[i] == 1):\n",
    "                self.TP += 1\n",
    "            elif (self.y_true[i] == 1 and self.y_pred[i] == 0):\n",
    "                self.FP += 1\n",
    "            elif (self.y_true[i] == 0 and self.y_pred[i] == 1):\n",
    "                self.FN += 1\n",
    "            elif (self.y_true[i] == 0 and self.y_pred[i] == 0):\n",
    "                self.TN += 1\n",
    "            \n",
    "\n",
    "    def get_precision(self):\n",
    "        # print(f\"Precision = {self.TP}/({self.TP}+{self.FP})\")\n",
    "        precision = self.TP/(self.TP + self.FP)\n",
    "        return precision\n",
    "\n",
    "    def get_recall(self):\n",
    "        # print(f\"Recall = {self.TP}/({self.TP}+{self.FN})\")\n",
    "        recall = self.TP/(self.TP + self.FN)\n",
    "        return recall\n",
    "\n",
    "    def get_fmeasure(self):\n",
    "        fmeasure = (2 * self.get_precision() * self.get_recall())/(self.get_precision() + self.get_recall())\n",
    "        return fmeasure\n",
    "\n",
    "    def get_accuracy(self):\n",
    "        accuracy = (self.TP + self.TN) /(self.TP + self.TN + self.FP + self.FN)\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, x, y):\n",
    "        self.n_samples, self.n_features = x.shape\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_classes = len(self.classes)\n",
    "\n",
    "        self.features = x\n",
    "        self.target = y.flatten()\n",
    "\n",
    "        self.classes = np.unique(y)\n",
    "        self.mean_data = np.zeros((self.n_classes, self.n_features), dtype=np.float64)\n",
    "        self.std_data = np.zeros((self.n_classes, self.n_features), dtype=np.float64)\n",
    "        self.prior_data = np.zeros(self.n_classes)\n",
    "        \n",
    "\n",
    "    def get_target(self):\n",
    "        print(np.mean(self.mean_data.flatten()))\n",
    "        print(np.mean(self.std_data.flatten()))\n",
    "        # print(self.features[0])\n",
    "        print(self.target.shape)\n",
    "        print(self.features.shape)\n",
    "        return self.target\n",
    "\n",
    "\n",
    "    def class_sep(self):\n",
    "        '''Separates spam and not spam rows'''\n",
    "\n",
    "        data = self.features\n",
    "        label = self.target\n",
    "\n",
    "        label = label.reshape(label.shape[0], 1)\n",
    "\n",
    "        spIdx_lst = np.where(~label.any(axis=1))[0]\n",
    "        notIdx_sp_lst = np.where(label.any(axis=1))[0]\n",
    "\n",
    "        d_list = data.tolist()\n",
    "        sp_list = []\n",
    "        not_sp_list = []\n",
    "\n",
    "        for index in spIdx_lst:\n",
    "            sp_list += [d_list[index]]\n",
    "\n",
    "        for index in notIdx_sp_lst:\n",
    "            not_sp_list += [d_list[index]]\n",
    "\n",
    "\n",
    "        sp_data = np.asarray(sp_list)\n",
    "        not_sp_data = np.asarray(not_sp_list)\n",
    "\n",
    "        self.mean_data[0, :] = sp_data.mean(axis=0)\n",
    "        self.std_data[0, :] = sp_data.std(axis=0)\n",
    "        self.prior_data[0] = sp_data.shape[0] / float(self.n_samples)\n",
    "\n",
    "        self.mean_data[1, :] = not_sp_data.mean(axis=0)\n",
    "        self.std_data[1, :] = not_sp_data.std(axis=0)\n",
    "        self.prior_data[1] = not_sp_data.shape[0] / float(self.n_samples)\n",
    "\n",
    "\n",
    "    def fit(self):\n",
    "\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            temp = self.features[self.target==c]\n",
    "            self.mean_data[idx, :] = temp.mean(axis=0)\n",
    "            self.std_data[idx, :] = temp.std(axis=0)\n",
    "            self.prior_data[idx] = temp.shape[0] / float(self.n_samples)\n",
    "\n",
    "\n",
    "    def get_stats(self):\n",
    "        # return np.sum(self.features.flatten())\n",
    "        print(self.mean_data.shape)\n",
    "        return np.sum(self.mean_data[:,0].flatten()), np.sum(self.mean_data[:,1].flatten())\n",
    "\n",
    "    \n",
    "    def calc_posterior1(self, x):\n",
    "        '''Chooses the class label based on which class probability is higher'''\n",
    "\n",
    "        posteriors = []\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            prior = np.log(self.prior_data[i])\n",
    "            n_log = np.log(self.norm_pdf(x, i))\n",
    "            n_log = np.nan_to_num(n_log, nan=10^-8, posinf=10^8, neginf=10^-14)\n",
    "            posterior = np.sum(n_log)\n",
    "            posterior = prior + posterior\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "\n",
    "    def calc_posterior(self, x):\n",
    "        '''Calculates posterior prob for each class'''\n",
    "\n",
    "        posteriors = []\n",
    "\n",
    "        for i in range(self.n_classes):\n",
    "            prior = self.prior_data[i]\n",
    "            n_pdf = self.norm_pdf(x, i)\n",
    "            n_pdf = np.prod(np.nan_to_num(n_pdf, nan=10^-8, posinf=10^8, neginf=10^-8))\n",
    "            posterior = prior * n_pdf\n",
    "            posteriors.append(posterior)\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        preds = [self.calc_posterior(i) for i in x]\n",
    "        return np.asarray(preds, dtype=np.float64)\n",
    "\n",
    "\n",
    "    def norm_pdf(self, data, c_idx):\n",
    "        '''Calculates norm pdf'''\n",
    "\n",
    "        mean = self.mean_data[c_idx]\n",
    "        std = self.std_data[c_idx]\n",
    "\n",
    "        numerator = np.exp(- (data-mean)**2 / (2 * (std**2)))\n",
    "        denominator = std * np.sqrt(2 * np.pi)\n",
    "\n",
    "        return numerator / denominator\n",
    "\n",
    "x_tr, y_tr, x_tt, y_tt = train_test_split(imp_data, train_size=2/3, random_state=0)\n",
    "print(type(y_tt))\n",
    "\n",
    "g_nb = NaiveBayes(x_tr, y_tr)\n",
    "g_nb.fit()\n",
    "\n",
    "predictions = g_nb.predict(x_tt)\n",
    "print(\"LEN: \", predictions)\n",
    "gb_ce = ClassifierEvaluation(y_tt, predictions)\n",
    "gb_ce.eval()\n",
    "# gb_ce.get_accuracy()\n",
    "print(\"Precision:\", gb_ce.get_precision())\n",
    "print(\"Recall:\", gb_ce.get_recall())\n",
    "print(\"F-measure\", gb_ce.get_fmeasure())\n",
    "print(\"Accuracy:\", gb_ce.get_accuracy())\n",
    "print()\n",
    "print()\n",
    "# g_nb.predict(x_tt)\n",
    "\n",
    "# predict(x_tr, y_tr)\n",
    "\n",
    "GaussNB = GaussianNB()\n",
    "GaussNB.fit(x_tr, y_tr)\n",
    "y_expect = y_tt\n",
    "y_predict = GaussNB.predict(x_tt)\n",
    "print(\"LEN: \", len(y_predict))\n",
    "print()\n",
    "accuracy_score(y_expect,y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 352
    }
   ],
   "source": [
    "0.0 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}