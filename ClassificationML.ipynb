{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "06bd6e6bb8b1a59ff9a8fa9390517002ce8a147f96d75124d9423e09c7a6bc86"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Q1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Y_pred: \n [[ 1.85394655]\n [ 3.0919826 ]\n [ 2.26662523]\n [ 1.02858919]\n [ 4.33001865]\n [ 1.85394655]\n [ 0.6159105 ]\n [-1.03480423]\n [ 1.44126787]\n [-1.44748291]]\nRMSE:  3.7013259176662716\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import operator\n",
    "from scipy.linalg import svd\n",
    "import scipy.stats as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as la\n",
    "\n",
    "x = np.array([[-2], [-5], [-3], [0], [-8], [-2], [1], [5], [-1], [6]])\n",
    "y = np.array([[1], [-4], [1], [3], [11], [5], [0], [-1], [-3], [1]])\n",
    "\n",
    "def lse_1(x,y):\n",
    "    biasF = np.ones(((len(x)), 1))\n",
    "    X = np.hstack((biasF, x))\n",
    "\n",
    "    w = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "\n",
    "    return w\n",
    "    \n",
    "\n",
    "w = lse_1(x,y)\n",
    "\n",
    "Y = np.zeros(shape=(10,1))\n",
    "\n",
    "for j in range(len(y)):\n",
    "    Y[j] = (w[1]*x[j] + w[0])\n",
    "\n",
    "print(\"Y_pred: \\n\", Y)\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(np.mean((predictions-targets)**2))\n",
    "\n",
    "rootmean = rmse(Y, y)\n",
    "print(\"RMSE: \", rmse(Y, y))"
   ]
  },
  {
   "source": [
    "Q2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "Q3"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(3067, 56)\n[[-0.06714524 -0.06606198 -0.05728751 ...  0.02439089  7.17883358\n  14.5667205 ]\n [-0.06822851 -0.05934572 -0.06476206 ... -0.04377916  0.07259631\n   1.64333473]\n [-0.06822851 -0.06822851 -0.06508704 ... -0.01523505  1.43751384\n   2.89992547]\n ...\n [-0.06822851 -0.06042898 -0.04862136 ...  0.13109278  4.18901424\n   7.90462306]\n [-0.06822851 -0.06140393 -0.06822851 ... -0.03021664  0.91754526\n   1.94664974]\n [-0.06822851 -0.06822851 -0.06822851 ... -0.05197949 -0.0248978\n   0.0292656 ]]\n[-6.82285127e-02 -6.81960147e-02 -6.81851820e-02 ...  9.91916071e+01\n  1.08139400e+02  1.08930185e+02]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from cv2 import VideoWriter, VideoWriter_fourcc\n",
    "import matplotlib.cm as cm\n",
    "from collections import Counter\n",
    "import glob\n",
    "from matplotlib.image import imread\n",
    "from enum import Enum\n",
    "from scipy.linalg import svd\n",
    "import scipy.stats as ss\n",
    "\n",
    "# For testing\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "imp_data = np.genfromtxt('spambase.data', delimiter=',')\n",
    "\n",
    "def train_test_split(data, train_size):\n",
    "    '''Splitting testing and training data'''\n",
    "\n",
    "    # Resetting random seed\n",
    "    np.random.seed(0)\n",
    "\n",
    "    n = len(data)\n",
    "\n",
    "    # Rows shuffled\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    # Calculates array index for splitting\n",
    "    spltIdx = int((2/3)*n)\n",
    "\n",
    "    # Training-validation data split\n",
    "    data_train, data_test = data[:spltIdx,:], data[spltIdx:,:]\n",
    "\n",
    "    # Training data\n",
    "    x_tr, y_tr = np.hsplit(data_train, [-1])\n",
    "    # Testing Data\n",
    "    x_tt, y_tt = np.hsplit(data_test, [-1])\n",
    "\n",
    "    # Separating class label from data\n",
    "    class_label_tr = data_train[:, -1].astype(int)\n",
    "    class_label_tr = class_label_tr.reshape(class_label_tr.shape[0], 1)\n",
    "    dataset_tr = data_train[:, :-1]\n",
    "\n",
    "    class_label_tt = data_test[:, -1].astype(int)\n",
    "    class_label_tt = class_label_tt.reshape(class_label_tt.shape[0], 1)\n",
    "    dataset_tt = data_test[:, :-1]\n",
    "\n",
    "    # Filtering features with low std\n",
    "    dataset_tr = std_filter(dataset_tr, 0.001)\n",
    "    dataset_tt = std_filter(dataset_tt, 0.001)\n",
    "\n",
    "    return class_label_tr, dataset_tr, class_label_tt, dataset_tt\n",
    "\n",
    "\n",
    "def std_filter(data, std_val):\n",
    "    '''Filters out features with low std'''\n",
    "\n",
    "    # Standardizing the matrix\n",
    "    standardized_mat = (data - np.mean(data)) / np.std(data)\n",
    "\n",
    "    std_mat = np.std(standardized_mat, axis = 0)\n",
    "    col_num = standardized_mat.shape[1]\n",
    "    low_idx = []\n",
    "\n",
    "    for i in range(col_num):\n",
    "        if std_mat[i] <= std_val:\n",
    "            low_idx.append(i)\n",
    "\n",
    "    temp = 0\n",
    "    for j in low_idx:\n",
    "        standardized_mat = np.delete(standardized_mat, j - temp, 1)\n",
    "        temp += 1\n",
    "\n",
    "    return standardized_mat\n",
    "\n",
    "\n",
    "def class_sep(data, label):\n",
    "    '''Separates spam and not spam rows'''\n",
    "\n",
    "    spIdx_lst = np.where(~label.any(axis=1))[0]\n",
    "    notIdx_sp_lst = np.where(label.any(axis=1))[0]\n",
    "\n",
    "    d_list = data.tolist()\n",
    "    sp_list = []\n",
    "    not_sp_list = []\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    for index in spIdx_lst:\n",
    "        sp_list += [d_list[index]]\n",
    "\n",
    "    for index in notIdx_sp_lst:\n",
    "        not_sp_list += [d_list[index]]\n",
    "\n",
    "    # print(sp_data)\n",
    "    sp_data = np.asarray(sp_list)\n",
    "    not_sp_data = np.asarray(not_sp_list)\n",
    "    return sp_data, not_sp_data\n",
    "\n",
    "\n",
    "def nb_fit(x, y):\n",
    "    n_samples, n_features = x.shape\n",
    "    print(y)\n",
    "    classes = np.unique(y)\n",
    "    print(classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def norm_pdf(data, class_idx):\n",
    "    mean = self._mean[class_idx]\n",
    "    var = self._var[class_idx]\n",
    "    numerator = np.exp(- (x-mean)**2 / (2 * var))\n",
    "    denominator = np.sqrt(2 * np.pi * var)\n",
    "    return numerator / denominator\n",
    "\n",
    "\n",
    "class_label_tr, dataset_tr, class_label_tt, dataset_tt = train_test_split(imp_data, 2/3)\n",
    "\n",
    "sp_data, not_sp_data = class_sep(dataset_tr, class_label_tr)\n",
    "\n",
    "nb_fit(sp_data, not_sp_data)\n",
    "\n",
    "\n",
    "\n",
    "# Training data\n",
    "# x_tr, y_tr = np.hsplit(dataset_tr, [-1])\n",
    "# Testing Data\n",
    "# x_tt, y_tt = np.hsplit(dataset_tt, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}